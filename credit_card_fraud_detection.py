# -*- coding: utf-8 -*-
"""Credit_Card_fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dbsw9A4bODfcWE_ZImrGbpDvu3uEWQmd

We import necessary libraries
"""

import numpy as np
#panda required to convert the csv(comma seperated)
#into structures data frames for operations
import pandas as pd
#training data
from sklearn.model_selection import train_test_split
#using logical regression model
from sklearn.linear_model import LogisticRegression
#to check accuracy?performance of model
from sklearn.metrics import accuracy_score

#adding the dataset to the panda datafram
credit_card_data=pd.read_csv('/content/creditcard.csv')

"""the table formed has the credit card information which since sensitive is converted through principle component analysis which convert them into numerical values
amount coloumn in dollars
in this the class coloumn shows whether it is Valid or fraudulent

1 is fraudulant
0 is valid
"""

#first few rows of the dataset
credit_card_data.head()

#last few rows of data,we can see transaction is between a 48 hour window
credit_card_data.tail()

#the information of the dataset we are working on

credit_card_data.info()

#checking the values that are missing in each coloumn
credit_card_data.isnull().sum()

#checking the distribution of valid transactions and fraudulant transaction
credit_card_data['Class'].value_counts()

"""This table is highly unbalanced

0------>Valid

1------>Fraudulent
"""

#seperating all the valid and frudulant transsactions
legit=credit_card_data[credit_card_data.Class==0]
fraud=credit_card_data[credit_card_data.Class==1]

print(legit.shape)
print(fraud.shape)

#Statistical measures of the data working on
#the % are percentiles
legit.Amount.describe()

fraud.Amount.describe()

#compare the values for both of these transactions
credit_card_data.groupby('Class').mean()

"""**Under sampling**
Buidling a sample dataset having similar distribution of normal transactions and fradulent transactions

Number of fraudulant trans=492
"""

legit_sample=legit.sample(n=492)

"""Concatanting two dataframes"""

new_dataset=pd.concat([legit_sample,fraud],axis=0)
#axis=1 the values will be added coloumn wise
#axis=0 the values will be added row wise

new_dataset.head()

new_dataset.tail()

new_dataset['Class'].value_counts()

new_dataset.groupby('Class').mean()

"""Splitting the data into Features and Targets"""

X=new_dataset.drop(columns='Class',axis=1)
Y=new_dataset['Class']

print(X)

print(Y)

"""Split the data into Training data and Testing data"""

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)

print(X.shape, X_train.shape, X_test.shape)

"""Model Training

Logistic Regression
"""

model = LogisticRegression()

# training the Logistic Regression Model with Training Data
model.fit(X_train, Y_train)

"""Model Evaluation

Accuracy Score
"""

# accuracy on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy on Training data : ', training_data_accuracy)

# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score on Test Data : ', test_data_accuracy)



"""The training and test accuracy is similar hence no

over fitting(traning acc more than test)

under fitting(test acc more than training)
"""